{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1\n",
    "https://github.com/kapumota/Actividades-CC0C2/blob/main/Cuadernos-CC0C2/Clase2/Counts-backoff-suavizado.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"all models are wrong\", \"a model is wrong\", \"some models are useful\"]\n",
    "\n",
    "vocab = {'<s>','</s>','a','all','are','model','models', 'some', 'useful', 'wrong'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'all models' with probability: 1.0\n",
      "'models are' with probability: 1.0\n",
      "'are wrong' with probability: 0.5\n",
      "'a model' with probability: 1.0\n",
      "'model is' with probability: 1.0\n",
      "'is wrong' with probability: 1.0\n",
      "'some models' with probability: 1.0\n",
      "'are useful' with probability: 0.5\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bigram_counts = defaultdict(int)\n",
    "unigram_counts = defaultdict(int)\n",
    "\n",
    "# Obtenemos los bigramas y unigramas\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram_counts[(words[i], words[i+1])] += 1\n",
    "        unigram_counts[words[i]] += 1\n",
    "    unigram_counts[words[-1]] += 1\n",
    "\n",
    "# Calculamos la probabilidad de los bigramas\n",
    "def bigram_probability(w1, w2):\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1] if unigram_counts[w1] > 0 else 0\n",
    "\n",
    "# Mostramos los bigramas con su probabilidad\n",
    "for bigram in bigram_counts:\n",
    "    print(f\"'{\" \".join(bigram)}' with probability: {bigram_probability(*bigram)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'all models' with probability: 0.2\n",
      "'models are' with probability: 0.2727272727272727\n",
      "'are wrong' with probability: 0.18181818181818182\n",
      "'a model' with probability: 0.2\n",
      "'model is' with probability: 0.2\n",
      "'is wrong' with probability: 0.2\n",
      "'some models' with probability: 0.2\n",
      "'are useful' with probability: 0.18181818181818182\n",
      "'a models' with probability: 0.1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bigram_counts = defaultdict(int)\n",
    "unigram_counts = defaultdict(int)\n",
    "\n",
    "# Obtenemos los bigramas y unigramas\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram_counts[(words[i], words[i+1])] += 1\n",
    "        unigram_counts[words[i]] += 1\n",
    "    unigram_counts[words[-1]] += 1\n",
    "\n",
    "beta = 1\n",
    "vocab_size = len(unigram_counts)\n",
    "\n",
    "# Calculamos la probabilidad de los bigramas con suavizado de Laplace\n",
    "def laplace_smoothing_bigram_probability(w1, w2, beta, vocab_size):\n",
    "    bigram_prob = (bigram_counts[(w1, w2)] + beta) / (unigram_counts[w1] + beta * vocab_size)\n",
    "    return bigram_prob\n",
    "\n",
    "# Mostramos los bigramas con su probabilidad\n",
    "for bigram in bigram_counts:\n",
    "    print(f\"'{\" \".join(bigram)}'\",\"with probability:\", laplace_smoothing_bigram_probability(bigram[0], bigram[1],beta, vocab_size))\n",
    "print(f\"'a models'\",\"with probability:\", laplace_smoothing_bigram_probability('a', 'models',beta, vocab_size))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'all models' k = 0.05 and with probability: 0.7241379310344828\n",
      "'models are' k = 0.05 and with probability: 0.8367346938775508\n",
      "'are wrong' k = 0.05 and with probability: 0.42857142857142855\n",
      "'a model' k = 0.05 and with probability: 0.7241379310344828\n",
      "'model is' k = 0.05 and with probability: 0.7241379310344828\n",
      "'is wrong' k = 0.05 and with probability: 0.7241379310344828\n",
      "'some models' k = 0.05 and with probability: 0.7241379310344828\n",
      "'are useful' k = 0.05 and with probability: 0.42857142857142855\n",
      "'a models' k = 0.05 and with probability: 0.034482758620689655\n",
      "'all models' k = 0.15 and with probability: 0.4893617021276596\n",
      "'models are' k = 0.15 and with probability: 0.6417910447761195\n",
      "'are wrong' k = 0.15 and with probability: 0.34328358208955223\n",
      "'a model' k = 0.15 and with probability: 0.4893617021276596\n",
      "'model is' k = 0.15 and with probability: 0.4893617021276596\n",
      "'is wrong' k = 0.15 and with probability: 0.4893617021276596\n",
      "'some models' k = 0.15 and with probability: 0.4893617021276596\n",
      "'are useful' k = 0.15 and with probability: 0.34328358208955223\n",
      "'a models' k = 0.15 and with probability: 0.06382978723404256\n",
      "'a models' k = 0.15 and with probability: 0.06382978723404256\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bigram_counts = defaultdict(int)\n",
    "unigram_counts = defaultdict(int)\n",
    "\n",
    "# Obtenemos los bigramas y unigramas\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram_counts[(words[i], words[i+1])] += 1\n",
    "        unigram_counts[words[i]] += 1\n",
    "    unigram_counts[words[-1]] += 1\n",
    "\n",
    "vocab_size = len(unigram_counts)\n",
    "\n",
    "# Calculamos la probabilidad de los bigramas con suavizado de Laplace\n",
    "def laplace_smoothing_bigram_probability_k(w1, w2, beta, vocab_size):\n",
    "    bigram_prob = (bigram_counts[(w1, w2)] + beta) / (unigram_counts[w1] + beta * vocab_size)\n",
    "    return bigram_prob\n",
    "\n",
    "# Mostramos los bigramas con su probabilidad\n",
    "for k in [0.05,0.15]:\n",
    "    for bigram in bigram_counts:\n",
    "        print(f\"'{\" \".join(bigram)}' k = {k} and with probability:\", laplace_smoothing_bigram_probability_k(bigram[0], bigram[1],k, vocab_size))\n",
    "    print(f\"'a models' k = {k} and with probability:\", laplace_smoothing_bigram_probability_k('a', 'models',k, vocab_size))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'all models' using backoff with probability: 1.0\n",
      "'models are' using backoff with probability: 1.0\n",
      "'are wrong' using backoff with probability: 0.5\n",
      "'a model' using backoff with probability: 1.0\n",
      "'model is' using backoff with probability: 1.0\n",
      "'is wrong' using backoff with probability: 1.0\n",
      "'some models' using backoff with probability: 1.0\n",
      "'are useful' using backoff with probability: 0.5\n",
      "'a models' using backoff with probability: 0.16666666666666666\n",
      "'a models' using backoff with probability: 0.16666666666666666\n",
      "'all models' using stupid backoff with probability: 1.0\n",
      "'models are' using stupid backoff with probability: 1.0\n",
      "'are wrong' using stupid backoff with probability: 0.5\n",
      "'a model' using stupid backoff with probability: 1.0\n",
      "'model is' using stupid backoff with probability: 1.0\n",
      "'is wrong' using stupid backoff with probability: 1.0\n",
      "'some models' using stupid backoff with probability: 1.0\n",
      "'are useful' using stupid backoff with probability: 0.5\n",
      "'a models' using stupid backoff with probability: 0.06666666666666667\n",
      "'a models' using backoff with probability: 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la probabilidad de los bigramas usando backoff\n",
    "def backoff_bigram_probability(w1, w2, unigram_counts, bigram_counts, total_words):\n",
    "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
    "    if bigram_count > 0:\n",
    "        return bigram_count / unigram_counts.get(w1, 0)\n",
    "    else:\n",
    "        return unigram_counts.get(w2, 0) / total_words\n",
    "\n",
    "# Calculamos la probabilidad de los bigramas usando stupid backoff con un alpha de 0.4\n",
    "def stupid_backoff_bigram_probability(w1, w2, unigram_counts, bigram_counts, total_words, alpha):\n",
    "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
    "    if bigram_count > 0:\n",
    "        return bigram_count / unigram_counts.get(w1, 0)\n",
    "    else:\n",
    "        return alpha * (unigram_counts.get(w2, 0) / total_words)\n",
    "\n",
    "\n",
    "total_words = sum(unigram_counts.values())\n",
    "\n",
    "# Mostramos los bigramas con su probabilidad\n",
    "for bigram in bigram_counts:\n",
    "        print(f\"'{\" \".join(bigram)}' using backoff with probability:\", backoff_bigram_probability(bigram[0], bigram[1],unigram_counts,bigram_counts, total_words))\n",
    "print(f\"'a models' using backoff with probability:\", backoff_bigram_probability('a', 'models',unigram_counts,bigram_counts, total_words))\n",
    "for bigram in bigram_counts:\n",
    "        print(f\"'{\" \".join(bigram)}' using stupid backoff with probability:\", stupid_backoff_bigram_probability(bigram[0], bigram[1],unigram_counts,bigram_counts, total_words,0.4))\n",
    "print(f\"'a models' using backoff with probability:\", stupid_backoff_bigram_probability('a', 'models',unigram_counts,bigram_counts, total_words,0.4))\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "# Calculamos la frecuencia de los ngramas\n",
    "def calculate_NC(ngram_counts: Dict[Tuple[str, ...], int]) -> Dict[int, int]:\n",
    "    count_of_counts = collections.Counter()\n",
    "    for count in ngram_counts.values():\n",
    "        count_of_counts[count] += 1\n",
    "    return count_of_counts\n",
    "\n",
    "# Ordenamos las frecuencias de los ngramas\n",
    "def sort_NC(NC: Dict[int, int]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    counts = np.array(list(NC.keys()))\n",
    "    frequencies = np.array([NC[count] for count in counts])\n",
    "    sorted_indices = np.argsort(counts)\n",
    "    return counts[sorted_indices], frequencies[sorted_indices]\n",
    "\n",
    "# Aplicamos el descuento de Good-Turing\n",
    "def good_turing_discounting(ngram_counts: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, ...], float]:\n",
    "    NC = calculate_NC(ngram_counts)\n",
    "    counts, frequencies = sort_NC(NC)\n",
    "    \n",
    "    # Ajuste de conteos\n",
    "    total_ngrams = sum(ngram_counts.values())\n",
    "    max_count = max(counts)\n",
    "    adjusted_counts = {}\n",
    "    \n",
    "    for ngram, count in ngram_counts.items():\n",
    "        if count < max_count:\n",
    "            Nc = NC[count]\n",
    "            Nc1 = NC.get(count + 1, 0)\n",
    "            if Nc > 0:\n",
    "                C_star = (count + 1) * (Nc1 / Nc)\n",
    "                adjusted_counts[ngram] = C_star\n",
    "            else:\n",
    "                adjusted_counts[ngram] = count\n",
    "        else:\n",
    "            adjusted_counts[ngram] = count  # Para conteos máximos, no ajustamos\n",
    "    return adjusted_counts\n",
    "\n",
    "adjusted_unigram_counts = good_turing_discounting(unigram_counts)\n",
    "print(calculate_NC(unigram_counts))\n",
    "print(sort_NC(unigram_counts))\n",
    "print(adjusted_unigram_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección y preparación del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "corpus = None\n",
    "# Leemos el corpus\n",
    "with open('Quijote.txt', encoding='utf-8-sig') as file:\n",
    "    corpus = file.read()\n",
    "'''Funcion que aplica stemming una palabra'''\n",
    "def stemming(word :str):\n",
    "    suffixes = ['ando', 'iendo', 'ar', 'er', 'ir', 'ado', 'ción', 'sión', 'mente', 'es', 's']\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "'''Funcion que tokeniza un corpus, además de aplicar stemming a las palabras y omitir stopwords'''\n",
    "def tokenize(corpus :str):\n",
    "    tokens = {}\n",
    "    tokens_list = []\n",
    "    stopwords = ['de', 'la', 'que', 'el', 'ella', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'uno', 'para', 'con', 'no', 'su', 'al', 'lo']\n",
    "    for line in corpus.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        line = line.lower()\n",
    "        for i in string.punctuation:\n",
    "            line = line.replace(i, \" \")\n",
    "        line = line.split(\" \")\n",
    "        for word in line:\n",
    "            if word == '' or word in stopwords:\n",
    "                continue\n",
    "            word = stemming(word)\n",
    "            tokens_list.append(word)\n",
    "            if tokens.get(word):\n",
    "                tokens[word] += 1\n",
    "            else:\n",
    "                tokens[word] = 1\n",
    "    return tokens, tokens_list\n",
    "\n",
    "'''Funcion que filtra los tokens que aparecen al menos k veces'''\n",
    "def filter_min_k(k : int, tokens: dict):\n",
    "    return {token: count for token, count in tokens[0].items() if count >= k} , [token for token in tokens[1] if tokens[0][token] >= k]\n",
    "\n",
    "tokens, tokens_list = filter_min_k(5,tokenize(corpus))\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "unigram_brown = {}\n",
    "\n",
    "# Obtenemos los unigramas\n",
    "for word in tokens_list:\n",
    "    if unigram_brown.get(word):\n",
    "        unigram_brown[word] += 1\n",
    "    else:\n",
    "        unigram_brown[word] = 1\n",
    "\n",
    "bigram_brown = {}\n",
    "words = tokens_list\n",
    "# Obtenemos los bigramas\n",
    "for i in range(len(words) - 1):\n",
    "    bigram = (words[i], words[i+1])\n",
    "    print(bigram)\n",
    "    if bigram_brown.get(bigram):\n",
    "        bigram_brown[bigram] += 1\n",
    "    else:\n",
    "        bigram_brown[bigram] = 1\n",
    "\n",
    "total_words = sum(unigram_brown.values())\n",
    "total_bigrams = sum(bigram_brown.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Brown Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.07564533591657645\n",
      "0.009696709469946016\n",
      "0.0\n",
      "{'ingenioso': 'C41', 'hidalgo': 'C41', 'don': 'C41', 'quijote': 'C41', 'mancha': 'C41', 'tasa': 'C41', 'yo': 'C41', 'juan': 'C41', 'gallo': 'C41', 'escribano': 'C41', 'cámara': 'C41', 'rey': 'C41', 'nuestro': 'C41', 'señor': 'C41', 'consejo': 'C41', 'doy': 'C41', 'fe': 'C41', 'hab': 'C41', 'visto': 'C41', 'dél': 'C41', 'libro': 'C41', 'intitul': 'C21', 'compuesto': 'C41', 'miguel': 'C41', 'cervant': 'C41', 'saavedra': 'C41', 'cada': 'C41', 'pliego': 'C41', 'dicho': 'C41', 'tr': 'C41', 'maravedí': 'C41', 'medio': 'C41', 'cual': 'C41', 'tiene': 'C41', 'ochenta': 'C34', 'precio': 'C41', 'monta': 'C41', 'dociento': 'C41', 'ha': 'C41', 'vend': 'C41', 'papel': 'C41', 'dieron': 'C41', 'licencia': 'C41', 'este': 'C41', 'pueda': 'C41', 'esta': 'C41', 'ponga': 'C41', 'principio': 'C41', 'sin': 'C41', 'dello': 'C41', 'di': 'C41', 'presente': 'C41', 'veinte': 'C41', 'día': 'C41', 'm': 'C41', 'mil': 'C41', 'seisciento': 'C41', 'cuatro': 'C41', 'año': 'C41', 'testimonio': 'C41', 'errata': 'C60', 'cosa': 'C41', 'digna': 'C41', 'corresponda': 'C63', 'original': 'C41', 'fee': 'C41', 'colegio': 'C66', 'madre': 'C41', 'dio': 'C41', 'universidad': 'C69', 'primero': 'C41', 'licenci': 'C41', 'llana': 'C41', 'cuanto': 'C41', 'parte': 'C41', 'vo': 'C41', 'no': 'C41', 'fue': 'C41', 'fecha': 'C41', 'rela': 'C41', 'o': 'C41', 'había': 'C41', 'cost': 'C41', 'mucho': 'C41', 'trabajo': 'C41', 'era': 'C41', 'muy': 'C41', 'útil': 'C87', 'provechoso': 'C41', 'd': 'C41', 'facultad': 'C90', 'le': 'C41', 'pod': 'C41', 'imprim': 'C41', 'tiempo': 'C41', 'fuésemo': 'C95', 'servido': 'C41', 'como': 'C41', 'nuestra': 'C41', 'merced': 'C41'}\n"
     ]
    }
   ],
   "source": [
    "class BrownClustering:\n",
    "    # Inicializamos la clase\n",
    "    def __init__(self, tokens: dict, bigrams: dict):\n",
    "        self.tokens = tokens\n",
    "        self.bigrams = bigrams\n",
    "        self.N = sum(tokens.values())\n",
    "        self.total_bigrams = sum(bigrams.values())\n",
    "        self.clusters = self.initialize_clusters()\n",
    "\n",
    "    # Inicializamos los clusters\n",
    "    def initialize_clusters(self):\n",
    "        return {word: 'C' + str(i) for i, word in enumerate(self.tokens.keys())}\n",
    "\n",
    "    # Calculamos la probabilidad de una palabra\n",
    "    def brown_P_word(self, word: str):\n",
    "        return self.tokens.get(word, 0) / self.N\n",
    "\n",
    "    # Calculamos la probabilidad de un bigrama\n",
    "    def brown_P_w1_w2(self, w1: str, w2: str):\n",
    "        bigram = (w1, w2)\n",
    "        return self.bigrams.get(bigram, 0) / self.total_bigrams\n",
    "\n",
    "    # Calculamos la probabilidad de un cluster\n",
    "    def brown_P_cluster(self, cluster: str):\n",
    "        s = 0.0\n",
    "        for word in self.clusters:\n",
    "            if self.clusters[word] == cluster:\n",
    "                s += self.brown_P_word(word)\n",
    "        return s\n",
    "\n",
    "    # Calculamos la probabilidad de dos clusters\n",
    "    def brown_P_cl1_cl2(self, cl1: str, cl2: str):\n",
    "        s = 0.0\n",
    "        for word1 in self.clusters:\n",
    "            if self.clusters[word1] == cl1:\n",
    "                for word2 in self.clusters:\n",
    "                    if self.clusters[word2] == cl2:\n",
    "                        s += self.brown_P_w1_w2(word1, word2)\n",
    "        return s\n",
    "\n",
    "    # Calcular el par de clusters óptimo\n",
    "    def search_optimal(self):\n",
    "        min_loss = float('inf')\n",
    "        cluster_pair = None\n",
    "        cluster_keys = list(set(self.clusters.values()))\n",
    "        num_clusters = len(cluster_keys)\n",
    "\n",
    "        for i in range(num_clusters):\n",
    "            for j in range(i + 1, num_clusters):\n",
    "                cl1 = cluster_keys[i]\n",
    "                cl2 = cluster_keys[j]\n",
    "                \n",
    "                P_cl1 = self.brown_P_cluster(cl1)\n",
    "                P_cl2 = self.brown_P_cluster(cl2)\n",
    "                P_cl1_cl2 = self.brown_P_cl1_cl2(cl1, cl2)\n",
    "                \n",
    "                loss = - (P_cl1 + P_cl2) + 2 * P_cl1_cl2\n",
    "                if loss < min_loss:\n",
    "                    min_loss = loss\n",
    "                    cluster_pair = (cl1, cl2)\n",
    "                    \n",
    "        return cluster_pair\n",
    "\n",
    "    # Cambiar el cluster de una palabra\n",
    "    def change_cluster(self, cl1: str, cl2: str):\n",
    "        for word in self.clusters:\n",
    "            if self.clusters[word] == cl1:\n",
    "                self.clusters[word] = cl2\n",
    "\n",
    "    # Realiza el clustering de Brown\n",
    "    def brown_clustering(self, final_clusters: int):\n",
    "        while len(set(self.clusters.values())) > final_clusters:\n",
    "            cluster_pair = self.search_optimal()\n",
    "            if cluster_pair is None:\n",
    "                break\n",
    "            self.change_cluster(*cluster_pair)\n",
    "        return self.clusters\n",
    "\n",
    "new_tokens = {}\n",
    "nTokens = 100\n",
    "counter = 0\n",
    "for word in tokens_list:\n",
    "    if not new_tokens.get(word):\n",
    "        new_tokens[word] = tokens[word]\n",
    "        counter += 1\n",
    "    if counter >= nTokens:\n",
    "        break\n",
    "        \n",
    "bc = BrownClustering(new_tokens, bigram_brown)\n",
    "\n",
    "final_clusters = 10\n",
    "clusters = bc.brown_clustering(final_clusters)\n",
    "# Mostramos los clusters\n",
    "print(bc.brown_P_cluster('C0'))\n",
    "print(bc.brown_P_word('don'))\n",
    "print(bc.brown_P_w1_w2('don', 'quijote'))\n",
    "print(bc.brown_P_cl1_cl2('C1', 'C2'))\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "class Word2VecCBOW:\n",
    "    # Inicializamos la clase para el CBOW\n",
    "    def __init__(self, vocab_size: int, dimension: int, window_size: int, alpha: float):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dimension = dimension\n",
    "        self.window_size = window_size\n",
    "        self.alpha = alpha\n",
    "        # Inicializamos los embeddings\n",
    "        self.word_embeddings = {i: [random.uniform(-0.5, 0.5) for _ in range(dimension)] for i in range(vocab_size)}\n",
    "        self.context_embeddings = {i: [random.uniform(-0.5, 0.5) for _ in range(dimension)] for i in range(vocab_size)}\n",
    "        # Definimos las operaciones\n",
    "        self.dot_product = lambda vec1, vec2: sum(x * y for x, y in zip(vec1, vec2))\n",
    "        self.sigmoid = lambda x: 1 / (1 + math.exp(-x))\n",
    "        self.scalar_multiply = lambda scalar, vector: [scalar * x for x in vector]\n",
    "        self.vector_add = lambda vec1, vec2: [x + y for x, y in zip(vec1, vec2)]\n",
    "        self.vector_subtract = lambda vec1, vec2: [x - y for x, y in zip(vec1, vec2)]\n",
    "\n",
    "    # Calculamos el forward del CBOW\n",
    "    def cbow_forward(self, context_words: List[int]) -> List[float]:\n",
    "        context_vector = [0.0] * self.dimension\n",
    "        for word in context_words:\n",
    "            context_vector = self.vector_add(context_vector, self.word_embeddings[word])\n",
    "        return [x / len(context_words) for x in context_vector]\n",
    "\n",
    "    # Calculamos la pérdida del CBOW\n",
    "    def cbow_loss(self, context_vector: List[float], target_word: int) -> float:\n",
    "        target_vector = self.context_embeddings[target_word]\n",
    "        score = self.dot_product(context_vector, target_vector)\n",
    "        predicted = self.sigmoid(score)\n",
    "        return -math.log(predicted)\n",
    "\n",
    "    # Calculamos la pérdida del muestreo negativo\n",
    "    def negative_sampling_loss(self, context_vector: List[float], negative_samples: List[int]) -> float:\n",
    "        return sum(-math.log(1 - self.sigmoid(self.dot_product(context_vector, self.context_embeddings[neg])))\n",
    "                   for neg in negative_samples)\n",
    "\n",
    "    # Muestreo negativo\n",
    "    def negative_sampling(self, num_samples: int, target_word: int) -> List[int]:\n",
    "        return [sample for sample in (random.randint(0, self.vocab_size - 1) for _ in range(num_samples))\n",
    "                if sample != target_word]\n",
    "\n",
    "    # Entrenamos el CBOW\n",
    "    def train_cbow(self, context_words: List[int], target_word: int, num_negative_samples: int):\n",
    "        context_vector = self.cbow_forward(context_words)\n",
    "        \n",
    "        positive_loss = self.cbow_loss(context_vector, target_word)\n",
    "\n",
    "        negative_samples = self.negative_sampling(num_negative_samples, target_word)\n",
    "        negative_loss = self.negative_sampling_loss(context_vector, negative_samples)\n",
    "\n",
    "        total_loss = positive_loss + negative_loss\n",
    "\n",
    "        self.update_embeddings_cbow(context_words, context_vector, target_word, negative_samples)\n",
    "        return total_loss\n",
    "    # Actualizamos los embeddings\n",
    "    def update_embeddings_cbow(self, context_words: List[int], context_vector: List[float], target_word: int, negative_samples: List[int]):\n",
    "        target_vector = self.context_embeddings[target_word]\n",
    "        gradient = self.sigmoid(self.dot_product(context_vector, target_vector)) - 1\n",
    "        update = lambda vec, grad, target: self.vector_subtract(vec, self.scalar_multiply(self.alpha * grad, target))\n",
    "\n",
    "        self.context_embeddings[target_word] = update(self.context_embeddings[target_word], gradient, context_vector)\n",
    "\n",
    "        for context in context_words:\n",
    "            self.word_embeddings[context] = update(self.word_embeddings[context], gradient, target_vector)\n",
    "\n",
    "        for neg in negative_samples:\n",
    "            negative_vector = self.context_embeddings[neg]\n",
    "            gradient_neg = self.sigmoid(self.dot_product(context_vector, negative_vector))\n",
    "            self.context_embeddings[neg] = update(self.context_embeddings[neg], gradient_neg, context_vector)\n",
    "\n",
    "class Word2VecSkipgram:\n",
    "    # Inicializamos la clase para el Skipgram\n",
    "    def __init__(self, vocab_size: int, dimension: int, window_size: int, alpha: float):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dimension = dimension\n",
    "        self.window_size = window_size\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Inicializamos los embeddings\n",
    "        self.word_embeddings = {i: [random.uniform(-0.5, 0.5) for _ in range(dimension)] for i in range(vocab_size)}\n",
    "        self.context_embeddings = {i: [random.uniform(-0.5, 0.5) for _ in range(dimension)] for i in range(vocab_size)}\n",
    "        # Definimos las operaciones\n",
    "        self.dot_product = lambda vec1, vec2: sum(x * y for x, y in zip(vec1, vec2))\n",
    "        self.sigmoid = lambda x: 1 / (1 + math.exp(-x))\n",
    "        self.scalar_multiply = lambda scalar, vector: [scalar * x for x in vector]\n",
    "        self.vector_add = lambda vec1, vec2: [x + y for x, y in zip(vec1, vec2)]\n",
    "        self.vector_subtract = lambda vec1, vec2: [x - y for x, y in zip(vec1, vec2)]\n",
    "\n",
    "    # Calculamos la pérdida del Skipgram\n",
    "    def skipgram_loss(self, target_vector: List[float], context_word: int) -> float:\n",
    "        context_vector = self.context_embeddings[context_word]\n",
    "        score = self.dot_product(target_vector, context_vector)\n",
    "        predicted = self.sigmoid(score)\n",
    "        return -math.log(predicted)\n",
    "\n",
    "    # Calculamos la pérdida del muestreo negativo\n",
    "    def negative_sampling_loss_skipgram(self, target_vector: List[float], negative_samples: List[int]) -> float:\n",
    "        return sum(-math.log(1 - self.sigmoid(self.dot_product(target_vector, self.context_embeddings[neg])))\n",
    "                   for neg in negative_samples)\n",
    "    # Muestreo negativo\n",
    "    def negative_sampling(self, num_samples: int, target_word: int) -> List[int]:\n",
    "        return [sample for sample in (random.randint(0, self.vocab_size - 1) for _ in range(num_samples))\n",
    "                if sample != target_word]\n",
    "    # Entrenamos el Skipgram\n",
    "    def train_skipgram(self, target_word: int, context_words: List[int], num_negative_samples: int):\n",
    "        target_vector = self.word_embeddings[target_word]\n",
    "        \n",
    "        positive_losses = [self.skipgram_loss(target_vector, context) for context in context_words]\n",
    "\n",
    "        negative_samples = self.negative_sampling(num_negative_samples, target_word)\n",
    "        negative_loss = self.negative_sampling_loss_skipgram(target_vector, negative_samples)\n",
    "\n",
    "        total_loss = sum(positive_losses) + negative_loss\n",
    "\n",
    "        self.update_embeddings_skipgram(target_word, context_words, negative_samples)\n",
    "        return total_loss\n",
    "\n",
    "    # Actualizamos los embeddings\n",
    "    def update_embeddings_skipgram(self, target_word: int, context_words: List[int], negative_samples: List[int]):\n",
    "        update = lambda vec, grad, target: self.vector_subtract(vec, self.scalar_multiply(self.alpha * grad, target))\n",
    "        \n",
    "        target_vector = self.word_embeddings[target_word]\n",
    "        for context in context_words:\n",
    "            context_vector = self.context_embeddings[context]\n",
    "            gradient = self.sigmoid(self.dot_product(target_vector, context_vector)) - 1\n",
    "            self.context_embeddings[context] = update(self.context_embeddings[context], gradient, target_vector)\n",
    "            self.word_embeddings[target_word] = update(self.word_embeddings[target_word], gradient, context_vector)\n",
    "\n",
    "        for neg in negative_samples:\n",
    "            negative_vector = self.context_embeddings[neg]\n",
    "            gradient_neg = self.sigmoid(self.dot_product(target_vector, negative_vector))\n",
    "            self.context_embeddings[neg] = update(self.context_embeddings[neg], gradient_neg, target_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "class GloVe:\n",
    "    # Inicializamos la clase\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, x_max: float, alpha: float, learning_rate: float, max_iter: int):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.x_max = x_max \n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        self.word_vectors = {i: [random.uniform(-0.5, 0.5) for _ in range(embedding_dim)] for i in range(vocab_size)}\n",
    "        self.context_vectors = {i: [random.uniform(-0.5, 0.5) for _ in range(embedding_dim)] for i in range(vocab_size)}\n",
    "        self.word_biases = {i: random.uniform(-0.5, 0.5) for i in range(vocab_size)}\n",
    "        self.context_biases = {i: random.uniform(-0.5, 0.5) for i in range(vocab_size)}\n",
    "        self.f_weight = lambda x: (x / x_max)**alpha if x < x_max else 1.0\n",
    "\n",
    "    # Construimos la matriz de coocurrencia\n",
    "    def build_cooccurrence_matrix(self, corpus: List[List[int]], window_size: int) -> Dict[Tuple[int, int], float]:\n",
    "        cooccurrence_matrix = defaultdict(float)\n",
    "        for sentence in corpus:\n",
    "            for i, target in enumerate(sentence):\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(len(sentence), i + window_size + 1)\n",
    "                for j in range(start, end):\n",
    "                    if i != j:\n",
    "                        context = sentence[j]\n",
    "                        cooccurrence_matrix[(target, context)] += 1 / abs(i - j)\n",
    "        return cooccurrence_matrix\n",
    "\n",
    "    # Calculamos la función de costo\n",
    "    def compute_cost(self, cooccurrence_matrix: Dict[Tuple[int, int], float]) -> float:\n",
    "        cost = 0.0\n",
    "        for (i, j), x_ij in cooccurrence_matrix.items():\n",
    "            w_i, w_j = self.word_vectors[i], self.context_vectors[j]\n",
    "            b_i, b_j = self.word_biases[i], self.context_biases[j]\n",
    "            dot_product = sum(w * c for w, c in zip(w_i, w_j))\n",
    "            log_x_ij = math.log(x_ij)\n",
    "            f_ij = self.f_weight(x_ij)\n",
    "            error = dot_product + b_i + b_j - log_x_ij\n",
    "            cost += f_ij * (error ** 2)\n",
    "        return cost\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    def train(self, cooccurrence_matrix: Dict[Tuple[int, int], float]):\n",
    "        for iteration in range(self.max_iter):\n",
    "            total_cost = 0.0\n",
    "            for (i, j), x_ij in cooccurrence_matrix.items():\n",
    "                w_i, w_j = self.word_vectors[i], self.context_vectors[j]\n",
    "                b_i, b_j = self.word_biases[i], self.context_biases[j]\n",
    "                \n",
    "                dot_product = sum(w * c for w, c in zip(w_i, w_j))\n",
    "                error = (dot_product + b_i + b_j - math.log(x_ij))\n",
    "                f_ij = self.f_weight(x_ij)\n",
    "                gradient = f_ij * error\n",
    "                \n",
    "                for k in range(self.embedding_dim):\n",
    "                    w_i[k] -= self.learning_rate * gradient * w_j[k]\n",
    "                    w_j[k] -= self.learning_rate * gradient * w_i[k]\n",
    "                \n",
    "                self.word_biases[i] -= self.learning_rate * gradient\n",
    "                self.context_biases[j] -= self.learning_rate * gradient\n",
    "                \n",
    "                total_cost += f_ij * (error ** 2)\n",
    "\n",
    "            print(f\"Iteración {iteration + 1}, Costo total: {total_cost}\")\n",
    "            if total_cost < 1e-5:\n",
    "                print(\"Convergencia alcanzada\")\n",
    "                break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
