{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install geoopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3IriqWRn8kW",
        "outputId": "d6e76427-11c2-4e0b-9056-5ba7306dcdb5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geoopt\n",
            "  Downloading geoopt-0.5.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from geoopt) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.0->geoopt) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->geoopt) (3.0.2)\n",
            "Downloading geoopt-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: geoopt\n",
            "Successfully installed geoopt-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHxi8Z4NxM-m",
        "outputId": "08fbafc1-fa97-4cdc-c9a3-e6d5c34e0a2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "biV4FqFPWXRN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import geoopt\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tree import Tree\n",
        "from collections import Counter\n",
        "\n",
        "def extract_sequences(tree):\n",
        "    words = tree.leaves()\n",
        "    labels = []\n",
        "\n",
        "    def traverse(t):\n",
        "        if isinstance(t, Tree):\n",
        "            if len(t) == 1 and isinstance(t[0], str):\n",
        "                labels.append(t.label())\n",
        "            else:\n",
        "                for child in t:\n",
        "                    traverse(child)\n",
        "\n",
        "    traverse(tree)\n",
        "    return words, labels\n",
        "\n",
        "trees = treebank.parsed_sents()\n",
        "word_sequences, label_sequences = [], []\n",
        "\n",
        "for tree in trees:\n",
        "    words, labels = extract_sequences(tree)\n",
        "    word_sequences.append(words)\n",
        "    label_sequences.append(labels)\n",
        "\n",
        "word_vocab = Counter(word for words in word_sequences for word in words)\n",
        "label_vocab = Counter(label for labels in label_sequences for label in labels)\n",
        "\n",
        "for words, labels in zip(word_sequences, label_sequences):\n",
        "    assert len(words) == len(labels), f\"Length mismatch: {len(words)} words, {len(labels)} labels\"\n",
        "\n",
        "word2idx = {word: i for i, word in enumerate(word_vocab.keys(), start=1)}\n",
        "label2idx = {label: i for i, label in enumerate(label_vocab.keys(), start=1)}\n",
        "\n",
        "word2idx['<PAD>'] = 0\n",
        "word2idx['<UNK>'] = len(word2idx)\n",
        "label2idx['<PAD>'] = 0\n",
        "label2idx['<UNK>'] = len(label2idx)\n",
        "\n",
        "def convert_to_indices(sequences, vocab):\n",
        "    return [[vocab.get(token, vocab['<UNK>']) for token in seq] for seq in sequences]\n",
        "\n",
        "word_sequences_idx = convert_to_indices(word_sequences, word2idx)\n",
        "label_sequences_idx = convert_to_indices(label_sequences, label2idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperbolicEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, curvature=1.0):\n",
        "        super(HyperbolicEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.manifold = geoopt.PoincareBall(c=curvature)\n",
        "\n",
        "    def forward(self, x):\n",
        "        euclidean_embeddings = self.embedding(x)\n",
        "        hyperbolic_embeddings = self.manifold.expmap0(euclidean_embeddings)\n",
        "        return hyperbolic_embeddings\n",
        "\n",
        "class HyperbolicRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, curvature=1.0):\n",
        "        super(HyperbolicRNN, self).__init__()\n",
        "        self.hyperbolic_embedding = HyperbolicEmbedding(vocab_size, embedding_dim, curvature)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.hyperbolic_embedding(x)\n",
        "        rnn_out, hidden = self.rnn(embedded)\n",
        "        output = self.fc(rnn_out)\n",
        "        return output\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "label_vocab_size = len(label2idx)\n",
        "embedding_dim = 16\n",
        "hidden_dim = 32\n",
        "curvature = 1.0\n",
        "\n",
        "model = HyperbolicRNN(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=label_vocab_size,\n",
        "    curvature=curvature\n",
        ")\n",
        "\n",
        "def pad_sequences(sequences, max_len, pad_value=0):\n",
        "    return [seq + [pad_value] * (max_len - len(seq)) for seq in sequences]\n",
        "\n",
        "max_len = max(len(seq) for seq in word_sequences_idx)\n",
        "word_sequences_padded = torch.tensor(pad_sequences(word_sequences_idx, max_len), dtype=torch.long)\n",
        "label_sequences_padded = torch.tensor(pad_sequences(label_sequences_idx, max_len), dtype=torch.long)\n",
        "\n",
        "train_data = list(zip(word_sequences_padded, label_sequences_padded))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n"
      ],
      "metadata": {
        "id": "ncJA-cNnvpmZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for input_seq, target_seq in train_data:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_seq.unsqueeze(0))\n",
        "        loss = criterion(output.view(-1, label_vocab_size), target_seq.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkJKbsGlvrkm",
        "outputId": "cedd2e62-691a-4644-dc52-ed8553d5213f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 4148.1001\n",
            "Epoch 2/50, Loss: 3154.3222\n",
            "Epoch 3/50, Loss: 2528.5448\n",
            "Epoch 4/50, Loss: 2092.5789\n",
            "Epoch 5/50, Loss: 1766.1781\n",
            "Epoch 6/50, Loss: 1509.7136\n",
            "Epoch 7/50, Loss: 1302.3645\n",
            "Epoch 8/50, Loss: 1132.3349\n",
            "Epoch 9/50, Loss: 990.5759\n",
            "Epoch 10/50, Loss: 870.2556\n",
            "Epoch 11/50, Loss: 766.6083\n",
            "Epoch 12/50, Loss: 676.2856\n",
            "Epoch 13/50, Loss: 596.9731\n",
            "Epoch 14/50, Loss: 526.9695\n",
            "Epoch 15/50, Loss: 464.7245\n",
            "Epoch 16/50, Loss: 409.4416\n",
            "Epoch 17/50, Loss: 360.3025\n",
            "Epoch 18/50, Loss: 316.8306\n",
            "Epoch 19/50, Loss: 278.5631\n",
            "Epoch 20/50, Loss: 245.1176\n",
            "Epoch 21/50, Loss: 215.8396\n",
            "Epoch 22/50, Loss: 189.8643\n",
            "Epoch 23/50, Loss: 167.1349\n",
            "Epoch 24/50, Loss: 146.9950\n",
            "Epoch 25/50, Loss: 130.1513\n",
            "Epoch 26/50, Loss: 115.1602\n",
            "Epoch 27/50, Loss: 101.2121\n",
            "Epoch 28/50, Loss: 90.7339\n",
            "Epoch 29/50, Loss: 80.5238\n",
            "Epoch 30/50, Loss: 74.3471\n",
            "Epoch 31/50, Loss: 67.1306\n",
            "Epoch 32/50, Loss: 60.6303\n",
            "Epoch 33/50, Loss: 56.9719\n",
            "Epoch 34/50, Loss: 52.6460\n",
            "Epoch 35/50, Loss: 47.9382\n",
            "Epoch 36/50, Loss: 46.5309\n",
            "Epoch 37/50, Loss: 42.1082\n",
            "Epoch 38/50, Loss: 39.4284\n",
            "Epoch 39/50, Loss: 41.2676\n",
            "Epoch 40/50, Loss: 36.2685\n",
            "Epoch 41/50, Loss: 32.7173\n",
            "Epoch 42/50, Loss: 32.7799\n",
            "Epoch 43/50, Loss: 32.3397\n",
            "Epoch 44/50, Loss: 31.0981\n",
            "Epoch 45/50, Loss: 30.0982\n",
            "Epoch 46/50, Loss: 30.7408\n",
            "Epoch 47/50, Loss: 27.0471\n",
            "Epoch 48/50, Loss: 27.3417\n",
            "Epoch 49/50, Loss: 26.3610\n",
            "Epoch 50/50, Loss: 24.6150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, input_seq):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_seq.unsqueeze(0))\n",
        "        predicted = output.argmax(dim=-1)\n",
        "    return predicted"
      ],
      "metadata": {
        "id": "lZQBaahbvsKm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_sequence = \"Christian are running in the park\"\n",
        "custom_sequence = custom_sequence.split(\" \")\n",
        "input_indices = [word2idx[word] for word in custom_sequence if word in word2idx]\n",
        "\n",
        "sequence_length = 6\n",
        "\n",
        "input_indices_padded = input_indices + [word2idx[\"<PAD>\"]] * (sequence_length - len(input_indices))\n",
        "\n",
        "input_indices_padded = input_indices_padded[:sequence_length]\n",
        "\n",
        "input_tensor = torch.tensor(input_indices_padded)\n",
        "\n",
        "predicted_labels = evaluate(model, input_tensor)\n",
        "\n",
        "print(\"Input Words:\", custom_sequence)\n",
        "print(\"Predicted Labels:\", [label for label, idx in label2idx.items() if idx in predicted_labels[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO73uKnKSmR7",
        "outputId": "3a0a4e08-7338-41bc-d85b-5d715af973df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Words: ['Christian', 'are', 'running', 'in', 'the', 'park']\n",
            "Predicted Labels: ['NNP', 'DT', 'NN', 'IN', 'VBG', 'PRP$']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IlRxnSi7S6hB"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}